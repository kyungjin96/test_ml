{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyungjin96/test_ml/blob/main/ml_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc7V1dkEj9zi"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))    \n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4  # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)  # f(x+h)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x)  # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val  # 값 복원\n",
        "        it.iternext()\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqh8-YLlihf4"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet:\n",
        "    \"\"\"\n",
        "    params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n",
        "    params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n",
        "    params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n",
        "    grad : 기울기를 보관하는 딕셔너리 변수(numerical_gradient()의 반환값)\n",
        "    grads['W1']은 1번째 층의 가중치의 기울기, grads['b1']은 1번째 층의 편향의 기울기.\n",
        "    grads['W2']은 2번째 층의 가중치의 기울기, grads['b2']은 2번째 층의 편향의 기울기.\n",
        "    \"\"\"\n",
        "    # 초기화를 수행한다.\n",
        "    def __init__(self, input_size, hidden_size, output_size,\n",
        "                 weight_init_std=0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "            np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    # 예측(추론)을 수행한다.\n",
        "    def predict(self, x):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        #sigmoid 활성화함수\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "       \n",
        "        # softmax y의 확률값 \n",
        "        return y\n",
        "\n",
        "    # 손실 함수의 값을 구한다.\n",
        "    # x : 입력데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        #결과의 확률값과 원핫인코딩된 값을 비교\n",
        "        return cross_entropy_error(y, t)\n",
        "\n",
        "    # 정확도를 구한다.\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # 가중치 매개변수의 기울기를 구한다.\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "    '''   '''\n",
        "    def gradient(self, x, t):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "        grads = {}\n",
        "        \n",
        "        batch_num = x.shape[0]\n",
        "        \n",
        "        # forward\n",
        "        #print(x.shape)\n",
        "        #print(W1.shape)\n",
        "        #print(b1.shape)\n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        # backward\n",
        "        dy = (y - t) / batch_num\n",
        "        grads['W2'] = np.dot(z1.T, dy)\n",
        "        grads['b2'] = np.sum(dy, axis=0)\n",
        "        \n",
        "        da1 = np.dot(dy, W2.T)\n",
        "        dz1 = sigmoid_grad(a1) * da1\n",
        "        grads['W1'] = np.dot(x.T, dz1)\n",
        "        grads['b1'] = np.sum(dz1, axis=0)\n",
        "\n",
        "        return grads    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY9VCI5-jjZt",
        "outputId": "e52199ac-64be-4c83-8dcc-3aab8807230e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784, 100)\n",
            "(100,)\n",
            "(100, 10)\n",
            "(10,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
        "\n",
        "print(net.params['W1'].shape)  # (784, 100)\n",
        "print(net.params['b1'].shape)  # (100,)\n",
        "print(net.params['W2'].shape)  # (100, 10)\n",
        "print(net.params['b2'].shape)  # (10,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2FfNMVyjkBf",
        "outputId": "18fc03f5-28cb-4fcb-9010-d4999fcbc023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 10)\n"
          ]
        }
      ],
      "source": [
        "x = np.random.rand(100, 784)  # 더미 입력 데이터(100장 분량)\n",
        "y = net.predict(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "hz19frZYjwYv",
        "outputId": "f2d66ca9-824c-401e-a1d3-9dac41b5867c"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3283c799c6bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 더미 정답 레이블(100장 분량)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 기울기 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 주의 : 실행하는데 아주 오래걸림\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 나중에 표시 필요\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d7bd8c34950b>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-5d6dfc8a7b90>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mfxh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# f(x-h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d7bd8c34950b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# 가중치 매개변수의 기울기를 구한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d7bd8c34950b>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# x : 입력데이터, t : 정답 레이블\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m#결과의 확률값과 원핫인코딩된 값을 비교\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d7bd8c34950b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#sigmoid 활성화함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "x = np.random.rand(100, 784)  # 더미 입력 데이터(100장 분량)\n",
        "t = np.random.rand(100, 10)   # 더미 정답 레이블(100장 분량)\n",
        "\n",
        "grads = net.numerical_gradient(x, t)  # 기울기 계산\n",
        "    # 주의 : 실행하는데 아주 오래걸림\n",
        "    # 나중에 표시 필요\n",
        "print(grads['W1'].shape)  # (784, 100)\n",
        "print(grads['b1'].shape)  # (100,)\n",
        "print(grads['W2'].shape)  # (100, 10)\n",
        "print(grads['b2'].shape)  # (10,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "DCxl7zWNkGA3",
        "outputId": "1faf15bf-4db1-4173-97ed-b3d7ab749a3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: overflow encountered in exp\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = tf.keras.datasets.mnist.load_data()\n",
        "t_train=tf.keras.utils.to_categorical(t_train)\n",
        "t_test=tf.keras.utils.to_categorical(t_test)\n",
        "x_train=x_train.reshape(60000,-1)\n",
        "x_test=x_test.reshape(10000,-1)\n",
        "\n",
        "train_loss_list = []\n",
        "\n",
        "# 하이퍼파라미터\n",
        "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100   # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 매개변수 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # 학습 경과 기록\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "# 그래프 그리기\n",
        "x = np.arange(len(train_loss_list))\n",
        "plt.plot(x, train_loss_list, label='train loss')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.ylim(0, 2.5)\n",
        "plt.legend()                   # 위\n",
        "#plt.legend(loc='lower right') # 아래\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "Pv-l9djQ5UjW",
        "outputId": "bffcfbeb-0c7f-4bee-9371-7479710fa95d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train acc, test acc | 0.18576666666666666, 0.195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train acc, test acc | 0.8518666666666667, 0.8545\n",
            "train acc, test acc | 0.8499166666666667, 0.8521\n",
            "train acc, test acc | 0.8642666666666666, 0.8718\n",
            "train acc, test acc | 0.8580333333333333, 0.8654\n",
            "train acc, test acc | 0.8556833333333334, 0.8632\n",
            "train acc, test acc | 0.8595, 0.8604\n",
            "train acc, test acc | 0.8704833333333334, 0.8724\n",
            "train acc, test acc | 0.8527, 0.8575\n",
            "train acc, test acc | 0.86335, 0.8664\n",
            "train acc, test acc | 0.86675, 0.8687\n",
            "train acc, test acc | 0.8494666666666667, 0.852\n",
            "train acc, test acc | 0.8735333333333334, 0.8777\n",
            "train acc, test acc | 0.8768, 0.8841\n",
            "train acc, test acc | 0.8691, 0.876\n",
            "train acc, test acc | 0.8793666666666666, 0.8863\n",
            "train acc, test acc | 0.8704666666666667, 0.8778\n",
            "\n",
            "\n",
            "\n",
            "훈련데이터와 시험 데이터에 대한 정확도 추이\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "(x_train, t_train), (x_test, t_test) = tf.keras.datasets.mnist.load_data()\n",
        "t_train=tf.keras.utils.to_categorical(t_train)\n",
        "t_test=tf.keras.utils.to_categorical(t_test)\n",
        "x_train=x_train.reshape(60000,-1)\n",
        "x_test=x_test.reshape(10000,-1)\n",
        "\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "# 하이퍼파라미터\n",
        "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100   # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 1에폭당 반복 수\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 매개변수 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # 학습 경과 기록\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    # 1에폭당 정확도 계산\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "print(\"\\n\\n\\n훈련데이터와 시험 데이터에 대한 정확도 추이\")\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()  #acc 값 보여주기 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "976YmcxxEvcg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ml_test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbzcelKbhwqzX5kEu42lwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}